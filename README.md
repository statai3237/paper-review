# paper-review 

## RESERVOIR TRANSFORMER

If some layers of the transformer are kept frozen - i.e., never updated after random initialization can we match the performance of fully learned transformers, while being more efficient ?
Answer is yes, they find that freezing layers may actually improve performance.


You can check this paper for details. 

<https://arxiv.org/pdf/2012.15045.pdf>

